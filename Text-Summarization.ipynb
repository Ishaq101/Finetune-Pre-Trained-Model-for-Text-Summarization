{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tarfile \n",
    "# datasource = tarfile.open('liputan6_data.tar.gz') \n",
    "# datasource.extractall('./dataset') \n",
    "# datasource.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canonical/train : 193,883\n",
      "canonical/dev : 10,972\n",
      "canonical/test : 10,972\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"canonical/train :\",f\"{len(os.listdir('./dataset/liputan6_data/canonical/train')):,.0f}\")\n",
    "print(\"canonical/dev :\",f\"{len(os.listdir('./dataset/liputan6_data/canonical/dev')):,.0f}\")\n",
    "print(\"canonical/test :\",f\"{len(os.listdir('./dataset/liputan6_data/canonical/test')):,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtreme/dev : 4,948\n",
      "xtreme/test : 3,862\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"xtreme/dev :\",f\"{len(os.listdir('./dataset/liputan6_data/xtreme/dev')):,.0f}\")\n",
    "print(\"xtreme/test :\",f\"{len(os.listdir('./dataset/liputan6_data/xtreme/test')):,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100002.json\n",
      "dict_keys(['id', 'url', 'clean_article', 'clean_summary', 'extractive_summary'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Liputan6.com, Jakarta : Perdana Menteri Jepang Junichiro Koizumi meminta maaf atas kekejaman tentara Jepang pada masa Perang Dunia II di Asia.',\n",
       " 'Permohonan maaf secara formal itu Koizumi utarakan di depan pemimpin negara-negara Asia dan Afrika dalam Konferensi Tingkat Tinggi Asia-Afrika di Jakarta Convention Center, Jumat (22/4).',\n",
       " 'Koizumi mengatakan, pada masa silam Jepang terlalu ambisius untuk berkuasa.',\n",
       " 'Ternyata, ambisi itu justru menimbulkan kerusakan dan penderitaan luar biasa bagi penduduk di sejumlah negara, terutama di Asia.',\n",
       " '\" Untuk itu Jepang minta maaf, \" kata Koizumi.',\n",
       " 'Selain meminta maaf, Koizumi berharap KAA dapat membantu mempererat persahabatan antara Jepang dan negara-negara di Asia serta Afrika.',\n",
       " 'Jepang, tambah Koizumi, siap memberikan sumbangan dan bantuan bagi negara-negara Asia dan Afrika yang membutuhkan.',\n",
       " 'Pernyataan Koizumi membuat heran para pengamat.',\n",
       " 'Menurut mereka, ini kejadian langka.',\n",
       " 'Mereka menduga, pernyataan Koizumi terkait dengan makin panasnya hubungan Jepang dan Cina akhir-akhir ini.',\n",
       " 'Jepang ingin meredakan ketegangan melalui negara-negara peserta KAA.',\n",
       " 'Hubungan Cina-Jepang memang terus memburuk sejak Jepang menyetujui penerbitan buku pelajaran sejarah nasional Jepang, awal April 2005.',\n",
       " 'Apalagi buku itu sama sekali tak menceritakan kekejaman tentara Jepang selama masa PD II di Asia dan Pasifik.',\n",
       " 'Istilah Negeri Samurai saat itu adalah Dai Toa Senso atau Perang Asia Timur Raya [ baca : Cina Menolak Meminta Maaf kepada Jepang ].(ICH/Tim Liputan 6 SCTV).']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import random \n",
    "import pandas as pd\n",
    "\n",
    "n = random.choices(os.listdir('./dataset/liputan6_data/canonical/dev'),k=1)[0]\n",
    "n = '2.json'\n",
    "\n",
    "n = os.listdir('./dataset/liputan6_data/canonical/train')[1]\n",
    "print(n)\n",
    "\n",
    "\n",
    "file = open(f'dataset/liputan6_data/canonical/train/{n}','r')\n",
    "mydict = json.load(file)\n",
    "print(mydict.keys())\n",
    "[' '.join(sent).replace(' .','.').replace(' ,',',').replace('( ','(').replace(' )',')').replace('. ','.') for sent in mydict['clean_article']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def get_article(json_filename_path,tobe_get_key='clean_article'):\n",
    "    file = open(json_filename_path,'r')\n",
    "    article_dict = json.load(file)\n",
    "    file.close()\n",
    "    return article_dict[tobe_get_key]\n",
    "\n",
    "def cleaning_article_sentence(list_sentence):\n",
    "    list_sentence_clean = [' '.join(sent).replace(' .','.').replace(' ,',',').replace('( ','(').replace(' )',')').replace('. ','.') for sent in list_sentence]\n",
    "    # remove \"Liputan6.com\" intro template\n",
    "    list_sentence_clean[0] = list_sentence_clean[0].split(':')[-1].strip()\n",
    "    # remove \"( xxxx )\" outro template\n",
    "    list_sentence_clean[-1] = list_sentence_clean[-1].split('.(')[0].strip()+'.'\n",
    "    pattern = '\\[.*\\]'\n",
    "    list_sentence_clean[-1] = re.sub(pattern,'',list_sentence_clean[-1]).replace(' .','.')\n",
    "    return list_sentence_clean\n",
    "\n",
    "def data_preparation(json_filename_path,tobe_get_key='clean_article'):\n",
    "    if tobe_get_key=='clean_article':\n",
    "        list_sentence = get_article(json_filename_path,tobe_get_key=tobe_get_key)\n",
    "        list_sentence = cleaning_article_sentence(list_sentence)\n",
    "        text = ' '.join(list_sentence)\n",
    "        return text\n",
    "    elif tobe_get_key=='clean_summary':\n",
    "        list_sentence = get_article(json_filename_path,tobe_get_key=tobe_get_key)\n",
    "        # print(list_sentence)\n",
    "        text = ' '.join([item for row in list_sentence for item in row])\n",
    "        text = text.replace(' .','.').replace(' ,',',')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dataset/liputan6_data/canonical/train'\n",
    "filenames_path = list(map(lambda x:path+'/'+x, os.listdir(path)))\n",
    "\n",
    "# articles = list(map(data_preparation,filenames_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 86.98 seconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "# Define the code block to be timed\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Create a semaphore to limit the number of parallel tasks\n",
    "semaphore = threading.BoundedSemaphore(5)\n",
    "\n",
    "# Iterate over the articles\n",
    "# for filename in filenames_path:\n",
    "    # Divide the item into chunks of size 700\n",
    "    \n",
    "    # Initialize a thread pool executor\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # Submit each chunk for summarization\n",
    "    future_articles = [executor.submit(data_preparation, filename, tobe_get_key='clean_article') for filename in filenames_path]\n",
    "    # Wait for all the tasks to complete and retrieve the results\n",
    "    articles = [future.result() for future in concurrent.futures.as_completed(future_articles)]\n",
    "    \n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    future_summaries = [executor.submit(data_preparation, filename, tobe_get_key='clean_summary') for filename in filenames_path]\n",
    "    summaries_ref = [future.result() for future in concurrent.futures.as_completed(future_summaries)]\n",
    "\n",
    "# Print the final summary for the item\n",
    "# final_summary = ' '.join(summaries)\n",
    "# print(final_summary)\n",
    "df_articles = pd.DataFrame(zip(articles,summaries_ref),columns=['articles','summary_ref'])\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>summary_ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jenazah Inspektur Satu Polisi Sugeng dimakamka...</td>\n",
       "      <td>Anggota DPRD Kota Makassar, Sulsel, dianggap t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tim search and rescue, Selasa (26/4), melanjut...</td>\n",
       "      <td>Penyegelan Kantor KPUD Padang Pariaman, Sumbar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sussongko Suhardjo, tersangka kasus dugaan pen...</td>\n",
       "      <td>Beberapa warga Mamasa meminta digelar jajak pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kota Medan dan beberapa wilayah di pesisir tim...</td>\n",
       "      <td>Subsidi BBM setiap bulan dilakukan untuk memba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ratusan warga Madura yang menjadi korban kerus...</td>\n",
       "      <td>Golok ini memiliki bobot dua ton dan panjang l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            articles  \\\n",
       "0  Jenazah Inspektur Satu Polisi Sugeng dimakamka...   \n",
       "1  Tim search and rescue, Selasa (26/4), melanjut...   \n",
       "2  Sussongko Suhardjo, tersangka kasus dugaan pen...   \n",
       "3  Kota Medan dan beberapa wilayah di pesisir tim...   \n",
       "4  Ratusan warga Madura yang menjadi korban kerus...   \n",
       "\n",
       "                                         summary_ref  \n",
       "0  Anggota DPRD Kota Makassar, Sulsel, dianggap t...  \n",
       "1  Penyegelan Kantor KPUD Padang Pariaman, Sumbar...  \n",
       "2  Beberapa warga Mamasa meminta digelar jajak pe...  \n",
       "3  Subsidi BBM setiap bulan dilakukan untuk memba...  \n",
       "4  Golok ini memiliki bobot dua ton dan panjang l...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 193883 entries, 0 to 193882\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   articles     193883 non-null  object\n",
      " 1   summary_ref  193883 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_articles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674c54abf733468b84c565cf8b53499d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bakmi\\.cache\\huggingface\\hub\\models--indolem--indobert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1ad775c8fa4fe3953f30b2e49104e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a49f4b0ceae41e8bc86ad30d3139fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/234k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5127d8283549f0afb8f502689ee874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940681ed180541818cd90f4e661562e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindolem/indobert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindolem/indobert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages\\transformers\\utils\\import_utils.py:1500\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1500\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages\\transformers\\utils\\import_utils.py:1488\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1486\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"indolem/indobert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.6/43.6 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bakmi\\anaconda3\\envs\\modeling\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/9.3 MB 7.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.6/9.3 MB 9.7 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.7/9.3 MB 6.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.0/9.3 MB 5.7 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.1/9.3 MB 5.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.3/9.3 MB 4.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.5/9.3 MB 4.6 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.6/9.3 MB 4.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.8/9.3 MB 4.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.9/9.3 MB 4.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.2/9.3 MB 4.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.3/9.3 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.5/9.3 MB 4.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.7/9.3 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.9/9.3 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.0/9.3 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.2/9.3 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.3/9.3 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.5/9.3 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.6/9.3 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.7/9.3 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.9/9.3 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.0/9.3 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.2/9.3 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.3/9.3 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.4/9.3 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.6/9.3 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.8/9.3 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.9/9.3 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.1/9.3 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.3/9.3 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.5/9.3 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.7/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.9/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.1/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.3/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.5/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.8/9.3 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.9/9.3 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.1/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.2/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.4/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.6/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.8/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.9/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.1/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.3/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.4/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.6/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.7/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.9/9.3 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.0/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "   ---------------------------------------- 0.0/402.6 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 153.6/402.6 kB 3.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 337.9/402.6 kB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 402.6/402.6 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading regex-2024.5.15-cp310-cp310-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/269.0 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 174.1/269.0 kB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.0/269.0 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp310-none-win_amd64.whl (287 kB)\n",
      "   ---------------------------------------- 0.0/287.4 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 174.1/287.4 kB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 287.4/287.4 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/2.2 MB 5.3 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.4/2.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.8/2.2 MB 4.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 4.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.2 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.2 MB 4.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.4 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.42.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
